from fastapi import FastAPI, Query
import google.generativeai as genai
from transformers import pipeline
import os
import httpx
import asyncio

from dotenv import load_dotenv
load_dotenv()

app = FastAPI()

cache = {}

NEWS_API_KEY = os.getenv("NEWS_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

genai.configure(api_key=GEMINI_API_KEY)

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

def clean_text(text):
    """
    Clean text by removing non-ASCII characters and trimming whitespace.
    """
    if text is None:
        return ""
    try:
        return text.encode("ascii", "ignore").decode().strip()
    except Exception as e:
        print(f"Error cleaning text: {e}")
        return text

def summarize_with_huggingface(text):
    """
    Use the Hugging Face transformer summarizer as a fallback.
    We limit the input to a maximum length so that the model works reliably.
    """
    try:
        limited_text = text if len(text) < 900 else text[:900]
        summary = summarizer(limited_text, max_length=100, min_length=70, do_sample=False)
        return summary[0]["summary_text"] if summary else "Summary not available."
    except Exception as e:
        print(f"Hugging Face summarization error: {e}")
        return "Summary not available."

def summarize_text(text):
    """
    Try summarizing using Google Gemini first. If that fails, use Hugging Face.
    Cache results to save repeated requests.
    """
    if not text.strip():
        return "No content available for summarization."
    if text in cache:
        print("Returning cached summary.")
        return cache[text]

    try:
        print("Attempting summarization using Gemini...")
        model = genai.GenerativeModel("gemini-2.0-flash")
        response = model.generate_content(f"Summarize this in 70 words: {text}")
        if hasattr(response, "text") and response.text.strip():
            summary = clean_text(response.text.strip())
            print("Summary generated by Gemini.")
            cache[text] = summary
            return summary if summary else "Summary not available."
    except Exception as e:
        print(f"Google Gemini summarization error: {e}")
    print("Falling back to Hugging Face for summarization...")
    hf_summary = summarize_with_huggingface(text)
    print("Summary generated by Hugging Face.")
    cache[text] = hf_summary
    return hf_summary

async def process_article(article: dict):
    """
    Process and summarize an individual article.
    Offload the summarization task to a thread to avoid blocking.
    """
    title = clean_text(article.get("title", "No Title"))
    description = clean_text(article.get("description", ""))
    content = clean_text(article.get("content", ""))
    full_text = f"{description} {content}".strip() or "No description available."

    summary = await asyncio.to_thread(summarize_text, full_text)

    return {
        "title": title,
        "summary": summary if summary not in ["Error generating summary.", "Summary not available."] else None,
        "image_url": article.get("urlToImage", ""),
        "article_url": article.get("url", "")
    }

@app.get("/news/basic")
async def get_basic_news():
    """
    Fast endpoint that returns basic news article info (without summaries).
    """
    url = (
        f"https://newsapi.org/v2/top-headlines?"
        f"sources=bbc-news,the-new-york-times,the-guardian-uk,cnn"
        f"language=en&pageSize=50&sortBy=publishedAt&apiKey={NEWS_API_KEY}"
    )
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(url)
            response.raise_for_status()
            news_data = response.json()
            articles = news_data.get("articles", [])
            basic_articles = [{
                "title": article.get("title", "No Title"),
                "description": article.get("description", ""),
                "image_url": article.get("urlToImage", ""),
                "article_url": article.get("url", "")
            } for article in articles]
            return {"status": "success", "articles": basic_articles}
        except Exception as e:
            print(f"Error fetching basic news: {e}")
            return {"status": "error", "message": "Could not fetch news."}

@app.get("/news/detailed")
async def get_detailed_news():
    """
    Endpoint that fetches news articles and concurrently processes them to add summaries.
    This may take longer due to summarization but provides detailed content.
    """
    url = (
        f"https://newsapi.org/v2/top-headlines?"
        f"sources=bbc-news,cnn,associated-press,buzzfeed,the-new-york-times,the-guardian-uk&"
        f"language=en&pageSize=40&sortBy=publishedAt&apiKey={NEWS_API_KEY}"
    )
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(url)
            response.raise_for_status()
            news_data = response.json()
            articles = news_data.get("articles", [])
        except Exception as e:
            print(f"Error fetching detailed news: {e}")
            return {"status": "error", "message": "Could not fetch news."}
    
    # Process articles concurrently.
    tasks = [process_article(article) for article in articles]
    results = await asyncio.gather(*tasks)
    return {"status": "success", "articles": results}

@app.get("/summarize")
async def get_summary(text: str = Query(..., description="Text to summarize")):
    """
    Endpoint to get a summary for arbitrary text.
    """
    summary = await asyncio.to_thread(summarize_text, text)
    return {"summary": summary}
